{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e06b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d7633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122b06f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a695f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b573585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e39019fa",
   "metadata": {},
   "source": [
    "**1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb8e2f",
   "metadata": {},
   "source": [
    "Here’s a breakdown of the differences, changes, and effects in each aspect you’re asking about:\n",
    "\n",
    "---\n",
    "\n",
    "1. **Simple Linear Regression vs. Multiple Linear Regression**\n",
    "   - **Simple Linear Regression** models the relationship between a single predictor (independent variable) and a response (dependent variable), using a straight line (y = mx + b) to show how one predictor affects the response.\n",
    "   - **Multiple Linear Regression** includes multiple predictors to model the response variable. The relationship becomes \\( y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n \\), allowing for a more complex and potentially accurate representation.  \n",
    "   - **Benefit of Multiple Linear Regression**: It can capture more of the factors influencing the response, improving model accuracy and explaining variability that a single predictor alone might miss.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Continuous Variable vs. Indicator Variable in Simple Linear Regression**\n",
    "   - **Continuous Variable**: Represents a range of values (e.g., age, weight) and implies a linear relationship where changes in the predictor correspond to proportional changes in the response.\n",
    "   - **Indicator Variable (or Dummy Variable)**: Represents categorical information with two levels, typically 0 and 1 (e.g., male or female). It implies different mean responses based on the category rather than a proportional relationship.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Introducing an Indicator Variable in Multiple Linear Regression**\n",
    "   - **Effect on Model Behavior**: When an indicator variable is added alongside a continuous variable, the model can now fit two parallel lines—one for each category in the indicator variable. This allows the model to account for different baseline levels of the response across categories while still capturing the continuous effect.\n",
    "   - **Comparison of Linear Forms**: In Simple Linear Regression, we only have one continuous predictor and one line. In Multiple Linear Regression with an indicator variable, we get different intercepts but the same slope for each category, effectively shifting the line up or down for each category.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Adding an Interaction Between a Continuous and Indicator Variable**\n",
    "   - **Effect on Model**: Adding an interaction term allows the effect of the continuous variable to change depending on the category indicated. The slopes are now different for each category, meaning each category will have a unique line fitted, adjusting both intercept and slope.\n",
    "   - **Form of Regression**: This is still a form of Multiple Linear Regression, but it includes an interaction term (\\( y = b_0 + b_1x_1 + b_2x_2 + b_3(x_1 \\times x_2) \\)) that enables each category to have its specific trend.\n",
    "\n",
    "---\n",
    "\n",
    "5. **Multiple Linear Regression with Only Indicator Variables from a Non-Binary Categorical Variable**\n",
    "   - **Expected Model Behavior**: The model uses the indicators to represent different categories by introducing binary (0/1) variables for each category (except one as a reference).\n",
    "   - **Form and Encoding**: This regression model assumes a different intercept for each category, and the response variable is modeled by shifting the intercept based on category rather than slope differences.\n",
    "   - **Binary Encoding Requirement**: Each category (except one) is represented by a binary variable, resulting in a set of mutually exclusive indicators (dummy variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd629b91",
   "metadata": {},
   "source": [
    "**history** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749e15a",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/6735a2cf-7f98-8003-8baf-b127a96aab35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc012dc",
   "metadata": {},
   "source": [
    "**2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0c45e",
   "metadata": {},
   "source": [
    "In this case, we're predicting **sales** based on two factors: **TV ad budget** and **online ad budget**. \n",
    "\n",
    "- **Without Interaction:** Each budget affects sales independently. Higher spending on TV or online ads separately boosts sales.\n",
    "  \n",
    "- **With Interaction:** We add a term that considers both budgets together, capturing the effect of increasing both at the same time. If having both budgets high works better together, this model would predict that increase more accurately.\n",
    "\n",
    "If budgets are **categorized as \"high\" or \"low\"**:\n",
    "- **Without Interaction:** Sales are based on whether each ad budget is high or low, without considering any joint effect.\n",
    "- **With Interaction:** This model includes a term for both being high, so it adjusts sales predictions if a high budget in both areas boosts sales more effectively together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e201dd84",
   "metadata": {},
   "source": [
    "**3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7d4b7",
   "metadata": {},
   "source": [
    "For your analysis with logistic regression on the Canadian Social Connection Survey dataset, here’s a structured approach based on the guidelines you’ve outlined:\n",
    "\n",
    "1. **Binary Outcome Variable:** \n",
    "   - Identify a categorical outcome variable in your dataset that you can convert to a binary format. For example, if you’re predicting \"socially connected\" vs. \"not socially connected,\" you could transform the categorical data into binary form using `.astype(int)` or by creating dummy variables.\n",
    "\n",
    "2. **Additive Model Specification:**\n",
    "   - Start with an **additive** model that includes the main effects of a couple of predictor variables, such as one continuous variable (e.g., \"age\") and one binary variable (e.g., \"participates in group activities\").\n",
    "   - The formula for an additive model in logistic regression could look like:\n",
    "     ```python\n",
    "     import statsmodels.formula.api as smf\n",
    "\n",
    "     # Example formula for additive specification\n",
    "     formula = 'social_connection ~ age + participates_in_groups'\n",
    "     model = smf.logit(formula, data=your_data).fit()\n",
    "     print(model.summary())\n",
    "     ```\n",
    "3. **Synergistic Interaction Specification:**\n",
    "   - Next, expand the formula to include an **interaction term** between your predictors, like `age * participates_in_groups`, which models how the effect of age on social connection might differ based on participation in group activities:\n",
    "     ```python\n",
    "     formula_interaction = 'social_connection ~ age * participates_in_groups'\n",
    "     model_interaction = smf.logit(formula_interaction, data=your_data).fit()\n",
    "     print(model_interaction.summary())\n",
    "     ```\n",
    "\n",
    "4. **Interpreting the Results:**\n",
    "   - Look at the coefficients in `.summary()` as though they were from a linear regression (even though they’re in log odds). Positive coefficients suggest that increases in that predictor increase the likelihood of the outcome, while negative coefficients suggest the opposite.\n",
    "   \n",
    "5. **Plotting (Pretending it’s Linear):**\n",
    "   - For visualization, we can approximate linear \"best fit lines\" for each model by generating scatter plots with simulated noise. Use Plotly to plot these \"best fit\" lines for both the additive and interaction models to observe how the relationship changes when including an interaction:\n",
    "     ```python\n",
    "     import plotly.graph_objects as go\n",
    "     import numpy as np\n",
    "\n",
    "     # Generate random data for predictors\n",
    "     np.random.seed(0)\n",
    "     age_values = np.random.normal(your_data['age'].mean(), your_data['age'].std(), 100)\n",
    "     group_values = np.random.choice([0, 1], 100)\n",
    "     \n",
    "     # Calculate \"best fit lines\" for additive and interaction models\n",
    "     pred_add = model.predict({'age': age_values, 'participates_in_groups': group_values})\n",
    "     pred_interaction = model_interaction.predict({'age': age_values, 'participates_in_groups': group_values})\n",
    "\n",
    "     # Plot additive model\n",
    "     fig_add = go.Figure()\n",
    "     fig_add.add_trace(go.Scatter(x=age_values, y=pred_add, mode='lines', name='Additive Model'))\n",
    "     fig_add.show()\n",
    "\n",
    "     # Plot interaction model\n",
    "     fig_inter = go.Figure()\n",
    "     fig_inter.add_trace(go.Scatter(x=age_values, y=pred_interaction, mode='lines', name='Interaction Model'))\n",
    "     fig_inter.show()\n",
    "     ```\n",
    "\n",
    "6. **Interpreting the Necessity of Interaction Terms:**\n",
    "   - Compare the additive and interaction models by looking at how the lines differ. If the interaction model shows a distinctly different pattern that matches the data better, it suggests the interaction term is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc9703",
   "metadata": {},
   "source": [
    "**4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57135d6",
   "metadata": {},
   "source": [
    "The apparent contradiction comes from two aspects of regression models:\n",
    "\n",
    "1. **Explained Variability (R-squared)**: When we say, “the model only explains 17.6% of the variability in the data,” we’re referring to the **R-squared** value. This value indicates how much of the outcome's variance (HP, in this case) can be attributed to the predictor variables in the model (Sp. Def and Generation). An R-squared of 17.6% means that **most of the variation in HP is not explained by the model** and that factors outside this model have a stronger impact on HP.\n",
    "\n",
    "2. **Statistical Significance of Coefficients**: The statement that \"many of the coefficients are larger than 10 while having strong or very strong evidence against the null hypothesis\" means that some of the predictor variables have a **statistically significant effect** on the outcome. In other words, even though the model doesn’t explain much of the overall variation in HP, the predictors still have an effect that is unlikely to be due to chance.\n",
    "\n",
    "### Why This Happens\n",
    "The coefficients can be statistically significant (indicating a reliable effect) without necessarily explaining a large portion of the outcome's variation. This often occurs when:\n",
    "- The predictors included in the model do have an impact on the outcome, but the **magnitude** of this effect isn’t large enough to account for most of the variability.\n",
    "- There might be **other variables** not included in the model that are important for predicting HP, and their absence limits the model’s R-squared.\n",
    "  \n",
    "Thus, **significant coefficients** show that changes in predictors like \"Sp. Def\" and \"Generation\" reliably affect HP, but the **low R-squared** tells us that these variables alone don’t capture all the important factors driving HP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ddcee",
   "metadata": {},
   "source": [
    "**5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53345e0",
   "metadata": {},
   "source": [
    "Let's break down each cell in this code to understand the data handling, model setup, and results.\n",
    "\n",
    "### Code Analysis\n",
    "\n",
    "1. **Setup and Splitting the Data**\n",
    "   ```python\n",
    "   fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "   pokeaman.fillna('None', inplace=True)\n",
    "   np.random.seed(130)\n",
    "   pokeaman_train, pokeaman_test = train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "   ```\n",
    "   - **Goal**: This section sets up the dataset for training and testing.\n",
    "   - **Explanation**:\n",
    "     - `fifty_fifty_split_size` calculates half of the dataset size for a 50-50 split.\n",
    "     - Missing values in the column `\"Type 2\"` are replaced with the string `'None'`.\n",
    "     - A seed (`np.random.seed(130)`) ensures reproducibility of the split.\n",
    "     - `train_test_split` splits the dataset into `pokeaman_train` and `pokeaman_test`.\n",
    "\n",
    "2. **Defining and Fitting Model 3 (Simple Model)**\n",
    "   ```python\n",
    "   model_spec3 = smf.ols(formula='HP ~ Attack + Defense', data=pokeaman_train)\n",
    "   model3_fit = model_spec3.fit()\n",
    "   model3_fit.summary()\n",
    "   ```\n",
    "   - **Goal**: Defines a linear regression model to predict HP based on `Attack` and `Defense` features.\n",
    "   - **Explanation**:\n",
    "     - `HP ~ Attack + Defense` specifies a simple model where `HP` is the dependent variable, and `Attack` and `Defense` are predictors.\n",
    "     - `model3_fit.summary()` provides detailed statistical information about the fit, including coefficients, R-squared, and p-values, giving insights into model quality and significance.\n",
    "\n",
    "3. **Evaluating Model 3's Performance**\n",
    "   ```python\n",
    "   yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "   y = pokeaman_test.HP\n",
    "   print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "   print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat_model3)[0,1]**2)\n",
    "   ```\n",
    "   - **Goal**: Evaluates Model 3 using R-squared values both for training (in-sample) and test (out-of-sample) data.\n",
    "   - **Explanation**:\n",
    "     - `model3_fit.rsquared` shows the in-sample R-squared, representing the model's ability to explain variance within the training data.\n",
    "     - For out-of-sample performance, `np.corrcoef(y, yhat_model3)[0,1]**2` calculates the R-squared based on the correlation between the actual and predicted `HP` values in the test set. This shows how well the model generalizes.\n",
    "\n",
    "4. **Defining Model 4 (Complex Model with Interaction Terms)**\n",
    "   ```python\n",
    "   model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "   model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "   ```\n",
    "   - **Goal**: Specifies a more complex model with interaction terms.\n",
    "   - **Explanation**:\n",
    "     - The formula includes `Attack`, `Defense`, `Speed`, `Legendary`, `Sp. Def`, and `Sp. Atk`, along with all possible interactions among them.\n",
    "     - The interaction terms (`*`) make the model account for relationships between pairs of features.\n",
    "     - `Q(\"Sp. Def\")` and `Q(\"Sp. Atk\")` enable referencing column names with spaces or special characters.\n",
    "\n",
    "   > **Note**: There’s a comment warning against adding categorical features (`Generation`, `Type 1`, `Type 2`) due to an overwhelming number of interactions that would likely make computation intractable.\n",
    "\n",
    "5. **Fitting and Evaluating Model 4**\n",
    "   ```python\n",
    "   model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "   model4_fit = model4_spec.fit()\n",
    "   model4_fit.summary()\n",
    "   yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "   y = pokeaman_test.HP\n",
    "   print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "   print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat_model4)[0,1]**2)\n",
    "   ```\n",
    "   - **Goal**: Fits and evaluates Model 4, similar to Model 3 but with added complexity.\n",
    "   - **Explanation**:\n",
    "     - `model4_fit.summary()` provides statistics for the model with interactions.\n",
    "     - The in-sample and out-of-sample R-squared calculations are performed as with Model 3, allowing for comparison between the simpler and more complex models.\n",
    "     - A higher in-sample R-squared in Model 4 compared to Model 3 could indicate better fit, but a significant difference between in-sample and out-of-sample R-squared might signal overfitting.\n",
    "\n",
    "### Interpretation of Results\n",
    "This code illustrates a step-by-step progression:\n",
    "1. **Basic Data Splitting and Preparation**: A standard data preparation workflow for splitting data into training and test sets, dealing with missing values, and ensuring reproducibility.\n",
    "2. **Model Complexity and Overfitting**:\n",
    "   - Model 3 is simple, predicting `HP` based on `Attack` and `Defense`.\n",
    "   - Model 4 is complex, including multiple features and their interactions. This model is more expressive but risks overfitting, especially if the out-of-sample R-squared drops significantly compared to the in-sample R-squared.\n",
    "3. **Evaluation**: R-squared in both models demonstrates how well each model captures the variance in `HP`:\n",
    "   - If Model 4’s out-of-sample R-squared is much lower than the in-sample R-squared, it suggests the model captures noise rather than meaningful patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0d74b",
   "metadata": {},
   "source": [
    "history https://chatgpt.com/share/6735ac9c-e744-8003-b90e-d2ad38472926"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04671f57",
   "metadata": {},
   "source": [
    "**6**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c89375",
   "metadata": {},
   "source": [
    "In your analysis with `model4_linear_form` for the Pokémon dataset, we see how the design matrix for a linear model can create issues in predictive stability and generalizability, especially due to multicollinearity. Let’s break down this situation and the role of centering and scaling in addressing it.\n",
    "\n",
    "### Understanding the Design Matrix and Predictor Variables\n",
    "The **design matrix** (`model4_spec.exog`) is the matrix of predictor variables, where each column represents a different feature or interaction term used in the linear model to predict the outcome variable (`model4_spec.endog`). When we include multiple features—especially interactions like `scale(center(Attack)) * scale(center(Defense)) * scale(center(Speed)) * Legendary`—we’re adding complexity to the matrix with many correlated variables.\n",
    "\n",
    "### Multicollinearity and the Condition Number\n",
    "Multicollinearity occurs when predictors are highly correlated with each other, which can lead to instability in the regression coefficients and reduces the model’s predictive power on new, unseen data. In matrix terms, a high **condition number** indicates multicollinearity. Here, even after centering and scaling the predictors, `model4_CS_fit` has an extremely high condition number (e.g., in the range of trillions), which signals severe multicollinearity. This is especially problematic in interaction terms involving multiple predictors.\n",
    "\n",
    "Without centering and scaling, `model3_fit` (a simpler model) already had a poor condition number of 343. By scaling and centering (`model3_center_scale_fit`), the condition number was reduced to around 1.66, drastically improving stability. However, for `model4_CS_fit`, even though centering and scaling were applied, the extensive interaction terms pushed the condition number back up to an astronomically high value, which means the design matrix remains poorly conditioned for this model.\n",
    "\n",
    "### Why Multicollinearity Affects Generalization\n",
    "With multicollinearity, the model becomes highly sensitive to minor fluctuations in the predictor variables, which can lead to overfitting. When testing on new data (out-of-sample), the model’s predictions might deviate significantly because of the amplified noise in the coefficient estimates due to multicollinearity. This instability is why `model4_fit` struggles to generalize beyond the training data.\n",
    "\n",
    "### In Summary\n",
    "For a model to generalize well, it’s essential to have a well-conditioned design matrix, typically achieved through centering, scaling, and controlling interaction terms. In `model4_linear_form`, even though predictors were scaled and centered, the intricate interaction terms led to a condition number that indicates severe multicollinearity, hindering the model’s ability to make reliable out-of-sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d07fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history https://chatgpt.com/share/6735ada0-995c-8003-9a16-1cf93b10e0ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180e484",
   "metadata": {},
   "source": [
    "**7**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4b408",
   "metadata": {},
   "source": [
    "The development of models from `model5_linear_form` through `model7_linear_form` represents a step-by-step refinement process in which each new model builds on the last by strategically adjusting the predictors and interactions to improve predictive accuracy and model stability. Here's how each model evolves:\n",
    "\n",
    "1. **Model 5 (`model5_linear_form`)**: This model includes a broad set of predictors, covering primary attributes (Attack, Defense, Speed) and other characteristics, like the legendary status and special stats, while also adding categorical indicators for `Generation` and `Type` variables. It serves as a baseline model with many predictors to capture various influences on HP.\n",
    "\n",
    "2. **Model 6 (`model6_linear_form`)**: Building on the significant variables from `model5`, this model removes some predictors that showed weaker associations (like `Defense`), focusing instead on predictors with higher influence. It then introduces binary indicators for specific values within `Generation` and `Type 1`, capturing finer distinctions and potentially reducing noise.\n",
    "\n",
    "3. **Model 7 (`model7_linear_form`)**: This model further refines by incorporating multiplicative (interaction) terms among key predictors (Attack, Speed, and the special stats), allowing it to account for combined effects that might affect HP more significantly than individual attributes alone.\n",
    "\n",
    "4. **Model 7 with Centering and Scaling (`model7_linear_form_CS`)**: This version improves the stability of `model7` by centering and scaling continuous predictors. This practice addresses issues of multicollinearity (as seen in the reduction of the condition number), enabling more reliable coefficient estimation without altering categorical indicators.\n",
    "\n",
    "**In summary:** Each model revision involves focusing on significant predictors, adding interactions, and improving stability through centering and scaling, progressively refining the model to better predict HP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d918fa",
   "metadata": {},
   "source": [
    "history https://chatgpt.com/share/6735ae7b-f0cc-8003-b259-567b3e48fbe8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8002f6f",
   "metadata": {},
   "source": [
    "**8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb979d80",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pokeaman_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m num_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Randomly split the data into training and test sets\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     train \u001b[38;5;241m=\u001b[39m \u001b[43mpokeaman_data\u001b[49m\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m     test \u001b[38;5;241m=\u001b[39m pokeaman_data\u001b[38;5;241m.\u001b[39mdrop(train\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Define and fit the model on the training data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pokeaman_data' is not defined"
     ]
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming pokeaman_data is our dataset\n",
    "# Lists to store in-sample and out-of-sample R-squared values\n",
    "in_sample_r_squared = []\n",
    "out_of_sample_r_squared = []\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = 50\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Randomly split the data into training and test sets\n",
    "    train = pokeaman_data.sample(frac=0.8, random_state=None)\n",
    "    test = pokeaman_data.drop(train.index)\n",
    "\n",
    "    # Define and fit the model on the training data\n",
    "    model_formula = 'HP ~ Attack + Defense + Speed + Legendary + Q(\"Sp. Def\") + Q(\"Sp. Atk\") + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
    "    model_spec = smf.ols(formula=model_formula, data=train)\n",
    "    model_fit = model_spec.fit()\n",
    "\n",
    "    # Calculate 'in sample' R-squared\n",
    "    in_sample_r_squared.append(model_fit.rsquared)\n",
    "\n",
    "    # Predict and calculate 'out of sample' R-squared on the test set\n",
    "    yhat_test = model_fit.predict(test)\n",
    "    y_test = test.HP\n",
    "    out_of_sample_r2 = np.corrcoef(y_test, yhat_test)[0, 1] ** 2\n",
    "    out_of_sample_r_squared.append(out_of_sample_r2)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(in_sample_r_squared, label='In-sample R-squared', marker='o')\n",
    "plt.plot(out_of_sample_r_squared, label='Out-of-sample R-squared', marker='x')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('R-squared')\n",
    "plt.legend()\n",
    "plt.title('In-Sample and Out-of-Sample R-squared over Multiple Model Runs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75321ff2",
   "metadata": {},
   "source": [
    "history https://chatgpt.com/share/6735ae7b-f0cc-8003-b259-567b3e48fbe8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "**9**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e9a1e1",
   "metadata": {},
   "source": [
    "In-sample R-squared: This measures the fit of the model to the data it was trained on.\n",
    "Out-of-sample R-squared: This evaluates the model’s ability to predict data it has not seen before (i.e., its generalization performance).\n",
    "Original model vs. Generation 1 model: The code compares these metrics for two models — one trained on all generations and another trained specifically on Generation 1 Pokémon data — to assess their predictive performance and generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa014d",
   "metadata": {},
   "source": [
    "In-sample R-squared: Measures how well each model fits its training data.\n",
    "For the original model, this is based on all generations.\n",
    "For the Generation 1 to 5 model, it’s based only on the data from generations 1 to 5.\n",
    "Out-of-sample R-squared: Measures how well each model predicts new, unseen data.\n",
    "For the original model, this is evaluated on the pokeaman_test data.\n",
    "For the Generation 1 to 5 model, this is evaluated on the Generation 6 data, which the model has not seen during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c069c4a8",
   "metadata": {},
   "source": [
    "This code is trying to assess the generalizability of the model trained only on Generation 1 Pokémon data. If the model trained on only Generation 1 data performs poorly when predicting for other generations, this may indicate that a more generalized model (trained on all generations) is preferable for making predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7006d148",
   "metadata": {},
   "source": [
    "This code is evaluating the generalizability of a model trained on only generations 1 to 5. If the out-of-sample performance on Generation 6 Pokémon is poor, this suggests that excluding data from Generation 6 reduces the model’s ability to make accurate predictions for all Pokémon generations. Conversely, a model that includes all generations (original model) would likely perform better in terms of generalization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade75e8b",
   "metadata": {},
   "source": [
    "history https://chatgpt.com/share/6735b149-1ea4-8003-ac51-50312404396a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435880ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
